{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f518c9-0a20-4573-bae8-ec0213e10e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1dfac5-d23d-4782-907b-724e864fdeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f929ea25-458c-4fab-baa2-e072df4552b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ef0631-78e7-4776-8334-1296d037a160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "training_instance_type = \"ml.g4dn.12xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86f5c4a-a555-4361-9fdd-74291277e1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "data_bucket = f\"<data-s3-bucket>\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/train/\"\n",
    "\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-tg-train\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b345e5ed-47e9-48e2-8fc9-67889101b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': '30', 'learning_rate': '6e-06', 'per_device_train_batch_size': '2', 'per_device_eval_batch_size': '8', 'warmup_ratio': '0.1', 'instruction_tuned': 'False', 'train_from_scratch': 'False', 'fp16': 'True', 'bf16': 'False', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '2', 'logging_steps': '10', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': True, 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"epoch\"] = \"30\"\n",
    "# hyperparameters[\"per_device_train_batch_size\"] = \"8\"\n",
    "hyperparameters[\"auto_find_batch_size\"] = True\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a018e7b-b172-493d-ab93-e2ab4f3452a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.0001, 0.001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa584ce5-ba8c-4945-a943-fefea945dc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-huggingface-textgener-2023-09-05-02-25-16-230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 02:25:16 Starting - Starting the training job...\n",
      "2023-09-05 02:25:42 Starting - Preparing the instances for training......\n",
      "2023-09-05 02:26:32 Downloading - Downloading input data..........................................................................\n",
      "2023-09-05 02:39:00 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:02,256 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:02,289 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:02,298 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:02,300 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:04,457 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.20.3-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.9.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface_hub/huggingface_hub-0.15.1-py3-none-any.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py_cpuinfo/py_cpuinfo-9.0.0-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.5-py2.py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 2)) (1.10.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.3->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.15.1->-r requirements.txt (line 3)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.15.1->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.15.1->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.15.1->-r requirements.txt (line 3)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3->-r requirements.txt (line 1)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.15.1->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.15.1->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.15.1->-r requirements.txt (line 3)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.15.1->-r requirements.txt (line 3)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3->-r requirements.txt (line 1)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mpy-cpuinfo is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.3-py3-none-any.whl size=843301 sha256=3e180034919ada698c77cb7188341533595b9567bee4c94e95ed9a4d55fe5b2b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e6/0e/61/48552c30a9cd82d794c8a1a914842ecd283612e0925d29314d\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, huggingface-hub, deepspeed, accelerate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.14.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.14.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.14.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 deepspeed-0.9.3 huggingface-hub-0.15.1 sagemaker-jumpstart-huggingface-script-utilities-1.0.5 sagemaker-jumpstart-script-utilities-1.1.8 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,063 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,063 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,097 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,139 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,181 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,190 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": true,\n",
      "        \"bf16\": \"False\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epoch\": \"30\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"20\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": \"True\",\n",
      "        \"gradient_accumulation_steps\": \"2\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"instruction_tuned\": \"False\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"6e-06\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"10\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"8\",\n",
      "        \"per_device_train_batch_size\": \"2\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": \"1\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"train_from_scratch\": \"False\",\n",
      "        \"validation_split_ratio\": \"0.2\",\n",
      "        \"warmup_ratio\": \"0.1\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"jumpstart-example-huggingface-textgener-2023-09-05-02-25-16-230\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-ap-southeast-2/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.2.0/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":true,\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"30\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"2\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-ap-southeast-2/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.2.0/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":true,\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"30\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"2\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"jumpstart-example-huggingface-textgener-2023-09-05-02-25-16-230\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-ap-southeast-2/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.2.0/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"True\",\"--bf16\",\"False\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epoch\",\"30\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"20\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--instruction_tuned\",\"False\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"6e-06\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"-1\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"2\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--train_from_scratch\",\"False\",\"--validation_split_ratio\",\"0.2\",\"--warmup_ratio\",\"0.1\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=true\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=30\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=True\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=False\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=6e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FROM_SCRATCH=False\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size True --bf16 False --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 30 --eval_accumulation_steps None --eval_steps 20 --evaluation_strategy steps --fp16 True --gradient_accumulation_steps 2 --gradient_checkpointing True --instruction_tuned False --label_smoothing_factor 0 --learning_rate 6e-06 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 10 --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_input_length -1 --max_steps -1 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 8 --per_device_train_batch_size 2 --preprocessing_num_workers None --save_steps 500 --save_strategy steps --save_total_limit 1 --seed 10 --train_data_split_seed 0 --train_from_scratch False --validation_split_ratio 0.2 --warmup_ratio 0.1 --warmup_steps 0 --weight_decay 0.2\u001b[0m\n",
      "\u001b[34m2023-09-05 02:39:17,220 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(model_dir='/opt/ml/model', train=None, train_alt='/opt/ml/input/data/train', validation=None, hosts=['algo-1'], num_gpus=4, current_host='algo-1', pretrained_model='/opt/ml/input/data/model', instruction_tuned='False', train_from_scratch='False', fp16='True', bf16='False', evaluation_strategy='steps', eval_steps=20, epoch=30, gradient_accumulation_steps=2, per_device_train_batch_size=2, per_device_eval_batch_size=8, logging_steps=10, warmup_ratio=0.1, learning_rate=6e-06, weight_decay=0.2, load_best_model_at_end='True', max_train_samples=-1, max_val_samples=-1, seed=10, max_input_length=-1, validation_split_ratio=0.2, train_data_split_seed=0, preprocessing_num_workers=None, max_steps=-1, gradient_checkpointing='True', early_stopping_patience=3, early_stopping_threshold=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_nan_inf_filter='True', save_strategy='steps', save_steps=500, save_total_limit=1, dataloader_drop_last='False', dataloader_num_workers=0, eval_accumulation_steps=None, auto_find_batch_size='True', lr_scheduler_type='constant_with_warmup', warmup_steps=0).\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /tmp. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Parameter 'instruction_tuned' is identified as 'False'. Domain adaption fine-tuning will be started.\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:12,123] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:12,154] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py --deepspeed ds_config.json --model_name_or_path /tmp --train_file /opt/ml/input/data/train --do_train --output_dir /opt/ml/model --num_train_epochs 30 --gradient_accumulation_steps 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --logging_steps 10 --warmup_ratio 0.1 --learning_rate 6e-06 --weight_decay 0.2 --seed 10 --max_input_length -1 --validation_split_ratio 0.2 --train_data_split_seed 0 --max_steps -1 --early_stopping_patience 3 --early_stopping_threshold 0.0 --adam_beta1 0.9 --adam_beta2 0.999 --max_grad_norm 1.0 --label_smoothing_factor 0.0 --logging_strategy steps --save_strategy steps --save_steps 500 --dataloader_num_workers 0 --lr_scheduler_type constant_with_warmup --warmup_steps 0 --evaluation_strategy steps --eval_steps 20 --load_best_model_at_end --fp16 --gradient_checkpointing --auto_find_batch_size --save_total_limit 1\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:163:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:14,313] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mSetting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,399] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,399] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,399] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,399] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,406] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,406] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,408] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,408] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:17,408] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:18 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:18 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:18 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:18 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=True,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=6e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Sep05_02-42-17_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant_with_warmup,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=30.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.2,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-09-05 02:42:18,586 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-09-05 02:42:18,657 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-09-05 02:42:18,657 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-09-05 02:42:18,674 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-09-05 02:42:18,674 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:18,683] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:18,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:18,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-09-05 02:42:18,690 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-09-05 02:42:18,690 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2023-09-05 02:42:18,691 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2023-09-05 02:42:18,691 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:18,691] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-09-05 02:42:18,694 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-09-05 02:42:18,694 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:27,464] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 6.05B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:10<00:20, 10.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:10<00:20, 10.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:10<00:20, 10.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:10<00:20, 10.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:20<00:10, 10.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:20<00:10, 10.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:20<00:10, 10.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:20<00:10, 10.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  7.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  8.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  7.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  8.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  7.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  8.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  7.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:24<00:00,  8.26s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-09-05 02:42:52,284 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-09-05 02:42:52,284 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-09-05 02:42:52,284 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2023-09-05 02:42:52,284 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-09-05 02:42:52,287 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-09-05 02:42:52,287 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|| 1/1 [00:00<00:00, 9137.92it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|| 1/1 [00:00<00:00, 1538.63it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:00<00:00, 824.35it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:00<00:00, 715.26it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:00<00:00, 440.30it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:00<00:00, 623.69it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:53 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|       | 6000/22325 [00:00<00:00, 53888.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|       | 5000/22325 [00:00<00:00, 40409.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|       | 5000/22325 [00:00<00:00, 41856.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|       | 6000/22325 [00:00<00:00, 43298.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|     | 11000/22325 [00:00<00:00, 45073.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|     | 11000/22325 [00:00<00:00, 41768.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  63%|   | 14000/22325 [00:00<00:00, 45009.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|     | 11000/22325 [00:00<00:00, 42254.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  72%|  | 16000/22325 [00:00<00:00, 43433.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  72%|  | 16000/22325 [00:00<00:00, 41685.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%| | 19000/22325 [00:00<00:00, 43686.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|  | 17000/22325 [00:00<00:00, 43001.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 22000/22325 [00:00<00:00, 45433.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 22000/22325 [00:00<00:00, 44227.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/22325 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|         | 2000/22325 [00:00<00:01, 17122.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|         | 2000/22325 [00:00<00:01, 18286.67 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|         | 2000/22325 [00:00<00:01, 18033.08 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|         | 2000/22325 [00:00<00:01, 18516.84 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  18%|        | 4000/22325 [00:00<00:01, 16345.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  18%|        | 4000/22325 [00:00<00:01, 16738.27 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  18%|        | 4000/22325 [00:00<00:01, 16648.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  18%|        | 4000/22325 [00:00<00:01, 16815.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|       | 6000/22325 [00:00<00:01, 16241.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|       | 6000/22325 [00:00<00:00, 16441.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|       | 6000/22325 [00:00<00:00, 16380.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|       | 6000/22325 [00:00<00:00, 16467.39 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  36%|      | 8000/22325 [00:00<00:00, 16164.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  36%|      | 8000/22325 [00:00<00:00, 16263.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  36%|      | 8000/22325 [00:00<00:00, 16242.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  36%|      | 8000/22325 [00:00<00:00, 16287.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|     | 10000/22325 [00:00<00:00, 16263.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|     | 10000/22325 [00:00<00:00, 16296.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|     | 10000/22325 [00:00<00:00, 16285.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|     | 10000/22325 [00:00<00:00, 16315.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|    | 12000/22325 [00:00<00:00, 16304.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|    | 12000/22325 [00:00<00:00, 16317.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|    | 12000/22325 [00:00<00:00, 16314.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|    | 12000/22325 [00:00<00:00, 16336.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  63%|   | 14000/22325 [00:00<00:00, 16304.73 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  63%|   | 14000/22325 [00:00<00:00, 16302.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  63%|   | 14000/22325 [00:00<00:00, 16292.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  63%|   | 14000/22325 [00:00<00:00, 16310.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  72%|  | 16000/22325 [00:00<00:00, 16341.07 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  72%|  | 16000/22325 [00:00<00:00, 16330.94 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  72%|  | 16000/22325 [00:00<00:00, 16317.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  72%|  | 16000/22325 [00:00<00:00, 16317.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  81%|  | 18000/22325 [00:01<00:00, 16108.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  81%|  | 18000/22325 [00:01<00:00, 16080.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  81%|  | 18000/22325 [00:01<00:00, 16077.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  81%|  | 18000/22325 [00:01<00:00, 16069.85 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%| | 20000/22325 [00:01<00:00, 16152.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%| | 20000/22325 [00:01<00:00, 16116.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%| | 20000/22325 [00:01<00:00, 16122.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%| | 20000/22325 [00:01<00:00, 16117.83 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|| 22000/22325 [00:01<00:00, 16088.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|| 22000/22325 [00:01<00:00, 16033.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|| 22000/22325 [00:01<00:00, 16044.69 examples/s]\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a1fe031ebe4e1639.arrow\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fa6cf896a2e6ada4.arrow and /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b37f16eda2fcb31d.arrow\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2023-09-05 02:42:55,221 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2023-09-05 02:42:55,221 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a1fe031ebe4e1639.arrow\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fa6cf896a2e6ada4.arrow and /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b37f16eda2fcb31d.arrow\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|| 22000/22325 [00:01<00:00, 16033.07 examples/s]\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a1fe031ebe4e1639.arrow\u001b[0m\n",
      "\u001b[34m09/05/2023 02:42:55 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fa6cf896a2e6ada4.arrow and /root/.cache/huggingface/datasets/text/default-d533e2ac0cae67ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b37f16eda2fcb31d.arrow\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,307] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,351] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,351] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,355] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,375] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:42:55,389] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 33.7512092590332 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 33.797789335250854 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 33.79425239562988 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 33.82032585144043 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000006, betas=(0.900000, 0.999000), weight_decay=0.200000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,491] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,507] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,507] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,507] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,507] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,632] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,633] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 1.26 GB         CA 0.39 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,633] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.47 GB, percent = 17.9%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,635] [INFO] [stage3.py:113:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:31,635] [INFO] [stage3.py:114:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.137057304382324 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.119745016098022 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.220366954803467 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.220187664031982 seconds\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,869] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,870] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,870] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.47 GB, percent = 17.9%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 811008 in 114 params\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,987] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,987] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:46,988] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.47 GB, percent = 17.9%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:47,082] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:47,083] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:47,083] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.47 GB, percent = 17.9%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,199] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,200] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,201] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.12 GB, percent = 25.8%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,540] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,542] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:50,542] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 56.22 GB, percent = 30.1%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:51,975] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:51,976] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:51,976] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.55 GB, percent = 34.6%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:52,248] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:52,250] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:52,250] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 72.57 GB, percent = 38.9%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:59,866] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:59,867] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.39 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:59,867] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 132.72 GB, percent = 71.1%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:43:59,867] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00036525726318359375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003528594970703125 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003418922424316406 seconds\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,309] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [utils.py:786:see_memory_usage] MA 0.14 GB         Max_MA 0.91 GB         CA 1.54 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 144.15 GB, percent = 77.3%\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbbda1ecee0>\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[6e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,311] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbc2508e380>\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,312] [INFO] [config.py:964:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 6e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.2}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 6e-06, 'warmup_num_steps': 48}\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   train_batch_size ............. 16\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,313] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:44:03,314] [INFO] [config.py:950:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 6e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.2\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 6e-06, \n",
      "            \"warmup_num_steps\": 48\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0002982616424560547 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2023-09-05 02:44:03,315 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2023-09-05 02:44:03,315 >>   Num examples = 252\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2023-09-05 02:44:03,315 >>   Num Epochs = 30\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2023-09-05 02:44:03,315 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2023-09-05 02:44:03,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2023-09-05 02:44:03,315 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2023-09-05 02:44:03,315 >>   Total optimization steps = 480\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2023-09-05 02:44:03,315 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2023-09-05 02:44:03,315 >>   Num examples = 252\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2023-09-05 02:44:03,315 >>   Num Epochs = 30\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2023-09-05 02:44:03,315 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2023-09-05 02:44:03,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2023-09-05 02:44:03,315 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2023-09-05 02:44:03,315 >>   Total optimization steps = 480\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2023-09-05 02:44:03,316 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2023-09-05 02:44:03,316 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m0%|          | 0/480 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/480 [00:38<5:07:25, 38.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/480 [01:12<4:47:52, 36.13s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/480 [01:46<4:37:21, 34.89s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/480 [02:20<4:35:26, 34.72s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/480 [02:55<4:35:16, 34.77s/it]\u001b[0m\n",
      "\u001b[34m1%|         | 6/480 [03:29<4:33:16, 34.59s/it]\u001b[0m\n",
      "\u001b[34m1%|         | 7/480 [04:04<4:32:26, 34.56s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 8/480 [04:39<4:32:15, 34.61s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 9/480 [05:12<4:29:20, 34.31s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 10/480 [05:47<4:29:38, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5852, 'learning_rate': 3.568791834636404e-06, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m2%|         | 10/480 [05:47<4:29:38, 34.42s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 11/480 [06:21<4:27:02, 34.16s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 12/480 [06:55<4:27:15, 34.26s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 13/480 [07:29<4:25:34, 34.12s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 14/480 [08:02<4:23:30, 33.93s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 15/480 [08:37<4:24:18, 34.10s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 16/480 [09:10<4:21:13, 33.78s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 17/480 [09:44<4:20:55, 33.81s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 18/480 [10:17<4:19:44, 33.73s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 19/480 [10:51<4:19:51, 33.82s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 20/480 [11:26<4:20:23, 33.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3801, 'learning_rate': 4.6431052251426525e-06, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m4%|         | 20/480 [11:26<4:20:23, 33.96s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 02:55:29,473 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 02:55:29,473 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 02:55:29,477 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 02:55:29,477 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 02:55:29,478 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 02:55:29,478 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.505859375, 'eval_runtime': 13.7324, 'eval_samples_per_second': 4.588, 'eval_steps_per_second': 0.146, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m4%|         | 20/480 [11:39<4:20:23, 33.96s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:06<00:00,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[2023-09-05 02:56:16,796] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m4%|         | 21/480 [12:13<4:50:33, 37.98s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 22/480 [12:47<4:41:16, 36.85s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 23/480 [13:21<4:33:48, 35.95s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 24/480 [13:55<4:27:39, 35.22s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 25/480 [14:28<4:23:52, 34.80s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 26/480 [15:02<4:20:36, 34.44s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 27/480 [15:36<4:18:31, 34.24s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 28/480 [16:09<4:16:36, 34.06s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 29/480 [16:43<4:14:41, 33.88s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 30/480 [17:17<4:14:56, 33.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1456, 'learning_rate': 5.2715382726114114e-06, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m6%|         | 30/480 [17:17<4:14:56, 33.99s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 31/480 [17:51<4:14:22, 33.99s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 32/480 [18:23<4:09:44, 33.45s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 33/480 [18:56<4:07:13, 33.19s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 34/480 [19:29<4:07:39, 33.32s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 35/480 [20:04<4:09:32, 33.65s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 36/480 [20:38<4:08:59, 33.65s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 37/480 [21:12<4:10:50, 33.97s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 38/480 [21:46<4:08:50, 33.78s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 39/480 [22:20<4:09:13, 33.91s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 40/480 [22:53<4:06:19, 33.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.791, 'learning_rate': 5.7174186156489e-06, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34m8%|         | 40/480 [22:53<4:06:19, 33.59s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:06:56,481 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:06:56,481 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:06:56,481 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:06:56,481 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:06:56,481 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:06:56,481 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.546875, 'eval_runtime': 14.9278, 'eval_samples_per_second': 4.22, 'eval_steps_per_second': 0.134, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34m8%|         | 40/480 [23:08<4:06:19, 33.59s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 41/480 [23:42<4:39:20, 38.18s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 42/480 [24:15<4:27:20, 36.62s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 43/480 [24:49<4:21:03, 35.84s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 44/480 [25:22<4:15:13, 35.12s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 45/480 [25:55<4:11:04, 34.63s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 46/480 [26:29<4:08:04, 34.30s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 47/480 [27:03<4:06:39, 34.18s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 48/480 [27:36<4:04:49, 34.00s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 49/480 [28:11<4:04:32, 34.04s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 50/480 [28:45<4:03:59, 34.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4953, 'learning_rate': 6e-06, 'epoch': 3.12}\u001b[0m\n",
      "\u001b[34m10%|         | 50/480 [28:45<4:03:59, 34.05s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 51/480 [29:19<4:04:51, 34.25s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 52/480 [29:52<4:01:16, 33.82s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 53/480 [30:25<3:59:27, 33.65s/it]\u001b[0m\n",
      "\u001b[34m11%|        | 54/480 [30:58<3:57:28, 33.45s/it]\u001b[0m\n",
      "\u001b[34m11%|        | 55/480 [31:32<3:57:50, 33.58s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 56/480 [32:06<3:58:00, 33.68s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 57/480 [32:40<3:56:49, 33.59s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 58/480 [33:14<3:58:31, 33.91s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 59/480 [33:47<3:55:29, 33.56s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 60/480 [34:20<3:54:04, 33.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1247, 'learning_rate': 6e-06, 'epoch': 3.75}\u001b[0m\n",
      "\u001b[34m12%|        | 60/480 [34:20<3:54:04, 33.44s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:18:24,029 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:18:24,029 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:18:24,029 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:18:24,029 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:18:24,029 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:18:24,029 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.80078125, 'eval_runtime': 13.0739, 'eval_samples_per_second': 4.819, 'eval_steps_per_second': 0.153, 'epoch': 3.75}\u001b[0m\n",
      "\u001b[34m12%|        | 60/480 [34:33<3:54:04, 33.44s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:06<00:00,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 61/480 [35:07<4:20:57, 37.37s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 62/480 [35:40<4:10:57, 36.02s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 63/480 [36:14<4:06:01, 35.40s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 64/480 [36:45<3:57:51, 34.31s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 65/480 [37:19<3:55:02, 33.98s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 66/480 [37:52<3:54:21, 33.96s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 67/480 [38:26<3:53:45, 33.96s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 68/480 [39:00<3:51:28, 33.71s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 69/480 [39:33<3:49:33, 33.51s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 70/480 [40:06<3:49:36, 33.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.813, 'learning_rate': 6e-06, 'epoch': 4.38}\u001b[0m\n",
      "\u001b[34m15%|        | 70/480 [40:06<3:49:36, 33.60s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 71/480 [40:41<3:50:27, 33.81s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 72/480 [41:15<3:50:55, 33.96s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 73/480 [41:49<3:51:12, 34.09s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 74/480 [42:23<3:50:40, 34.09s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 75/480 [42:57<3:48:19, 33.83s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 76/480 [43:30<3:46:51, 33.69s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 77/480 [44:03<3:45:09, 33.52s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 78/480 [44:37<3:45:42, 33.69s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 79/480 [45:11<3:45:39, 33.76s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 80/480 [45:43<3:41:15, 33.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5367, 'learning_rate': 6e-06, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m17%|        | 80/480 [45:43<3:41:15, 33.19s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:29:46,920 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:29:46,920 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:29:46,920 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:29:46,920 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:29:46,920 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:29:46,920 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.037109375, 'eval_runtime': 14.8092, 'eval_samples_per_second': 4.254, 'eval_steps_per_second': 0.135, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m17%|        | 80/480 [45:58<3:41:15, 33.19s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 81/480 [46:30<4:09:01, 37.45s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 82/480 [47:04<4:00:54, 36.32s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 83/480 [47:38<3:56:02, 35.67s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 84/480 [48:12<3:52:09, 35.17s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 85/480 [48:45<3:46:48, 34.45s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 86/480 [49:18<3:43:13, 33.99s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 87/480 [49:51<3:40:52, 33.72s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 88/480 [50:27<3:43:55, 34.27s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 89/480 [51:00<3:41:50, 34.04s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 90/480 [51:34<3:40:39, 33.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2545, 'learning_rate': 6e-06, 'epoch': 5.62}\u001b[0m\n",
      "\u001b[34m19%|        | 90/480 [51:34<3:40:39, 33.95s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 91/480 [52:08<3:40:21, 33.99s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 92/480 [52:41<3:38:24, 33.77s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 93/480 [53:16<3:38:53, 33.94s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 94/480 [53:49<3:37:57, 33.88s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 95/480 [54:23<3:36:44, 33.78s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 96/480 [54:56<3:34:23, 33.50s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 97/480 [55:29<3:34:02, 33.53s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 98/480 [56:04<3:34:58, 33.76s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 99/480 [56:37<3:33:53, 33.68s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 100/480 [57:11<3:33:26, 33.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1662, 'learning_rate': 6e-06, 'epoch': 6.25}\u001b[0m\n",
      "\u001b[34m21%|        | 100/480 [57:11<3:33:26, 33.70s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:41:14,706 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:41:14,706 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:41:14,709 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:41:14,709 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:41:14,709 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:41:14,709 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.6328125, 'eval_runtime': 14.9316, 'eval_samples_per_second': 4.219, 'eval_steps_per_second': 0.134, 'epoch': 6.25}\u001b[0m\n",
      "\u001b[34m21%|        | 100/480 [57:26<3:33:26, 33.70s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 101/480 [58:01<4:03:12, 38.50s/it]\u001b[0m\n",
      "\u001b[34m21%|       | 102/480 [58:34<3:53:31, 37.07s/it]\u001b[0m\n",
      "\u001b[34m21%|       | 103/480 [59:08<3:46:38, 36.07s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 104/480 [59:43<3:43:07, 35.61s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 105/480 [1:00:16<3:39:22, 35.10s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 106/480 [1:00:50<3:35:57, 34.65s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 107/480 [1:01:23<3:31:47, 34.07s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 108/480 [1:01:56<3:29:11, 33.74s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 109/480 [1:02:29<3:28:18, 33.69s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 110/480 [1:03:03<3:26:53, 33.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0865, 'learning_rate': 6e-06, 'epoch': 6.88}\u001b[0m\n",
      "\u001b[34m23%|       | 110/480 [1:03:03<3:26:53, 33.55s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 111/480 [1:03:36<3:25:40, 33.44s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 112/480 [1:04:08<3:23:42, 33.21s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 113/480 [1:04:42<3:24:06, 33.37s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 114/480 [1:05:15<3:22:39, 33.22s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 115/480 [1:05:48<3:22:25, 33.27s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 116/480 [1:06:22<3:22:30, 33.38s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 117/480 [1:06:55<3:21:20, 33.28s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 118/480 [1:07:27<3:18:52, 32.96s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 119/480 [1:08:01<3:18:47, 33.04s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 120/480 [1:08:33<3:18:04, 33.01s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0505, 'learning_rate': 6e-06, 'epoch': 7.5}\u001b[0m\n",
      "\u001b[34m25%|       | 120/480 [1:08:33<3:18:04, 33.01s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:52:37,327 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 03:52:37,327 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:52:37,328 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 03:52:37,328 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:52:37,328 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 03:52:37,328 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.35s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.75390625, 'eval_runtime': 13.5098, 'eval_samples_per_second': 4.663, 'eval_steps_per_second': 0.148, 'epoch': 7.5}\u001b[0m\n",
      "\u001b[34m25%|       | 120/480 [1:08:47<3:18:04, 33.01s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.35s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 121/480 [1:09:21<3:44:27, 37.51s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 122/480 [1:09:54<3:34:17, 35.91s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 123/480 [1:10:27<3:29:21, 35.19s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 124/480 [1:11:00<3:24:57, 34.54s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 125/480 [1:11:34<3:23:33, 34.40s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 126/480 [1:12:08<3:22:12, 34.27s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 127/480 [1:12:42<3:21:03, 34.17s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 128/480 [1:13:14<3:17:08, 33.60s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 129/480 [1:13:49<3:17:46, 33.81s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 130/480 [1:14:23<3:18:13, 33.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0399, 'learning_rate': 6e-06, 'epoch': 8.12}\u001b[0m\n",
      "\u001b[34m27%|       | 130/480 [1:14:23<3:18:13, 33.98s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 131/480 [1:14:56<3:16:08, 33.72s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 132/480 [1:15:31<3:16:46, 33.93s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 133/480 [1:16:05<3:16:11, 33.92s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 134/480 [1:16:39<3:15:44, 33.94s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 135/480 [1:17:13<3:16:39, 34.20s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 136/480 [1:17:47<3:14:25, 33.91s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 137/480 [1:18:20<3:12:46, 33.72s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 138/480 [1:18:53<3:11:58, 33.68s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 139/480 [1:19:27<3:10:31, 33.52s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 140/480 [1:20:00<3:09:43, 33.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0287, 'learning_rate': 6e-06, 'epoch': 8.75}\u001b[0m\n",
      "\u001b[34m29%|       | 140/480 [1:20:00<3:09:43, 33.48s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:04:03,859 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:04:03,859 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:04:03,861 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:04:03,861 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:04:03,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:04:03,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.66s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.8203125, 'eval_runtime': 14.755, 'eval_samples_per_second': 4.27, 'eval_steps_per_second': 0.136, 'epoch': 8.75}\u001b[0m\n",
      "\u001b[34m29%|       | 140/480 [1:20:15<3:09:43, 33.48s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.66s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 141/480 [1:20:48<3:34:36, 37.98s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 142/480 [1:21:23<3:27:44, 36.88s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 143/480 [1:21:56<3:21:06, 35.81s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 144/480 [1:22:28<3:14:30, 34.73s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 145/480 [1:23:02<3:11:59, 34.39s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 146/480 [1:23:35<3:09:06, 33.97s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 147/480 [1:24:09<3:08:09, 33.90s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 148/480 [1:24:43<3:07:57, 33.97s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 149/480 [1:25:17<3:07:29, 33.99s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 150/480 [1:25:50<3:05:47, 33.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0213, 'learning_rate': 6e-06, 'epoch': 9.38}\u001b[0m\n",
      "\u001b[34m31%|      | 150/480 [1:25:50<3:05:47, 33.78s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 151/480 [1:26:25<3:06:17, 33.97s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 152/480 [1:26:59<3:06:03, 34.03s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 153/480 [1:27:32<3:04:47, 33.91s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 154/480 [1:28:07<3:05:04, 34.06s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 155/480 [1:28:40<3:02:46, 33.74s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 156/480 [1:29:13<3:00:56, 33.51s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 157/480 [1:29:45<2:59:06, 33.27s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 158/480 [1:30:19<2:59:11, 33.39s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 159/480 [1:30:52<2:58:22, 33.34s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 160/480 [1:31:24<2:54:36, 32.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0193, 'learning_rate': 6e-06, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m33%|      | 160/480 [1:31:24<2:54:36, 32.74s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:15:27,494 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:15:27,494 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:15:27,495 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:15:27,495 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:15:27,495 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:15:27,495 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.853515625, 'eval_runtime': 13.5751, 'eval_samples_per_second': 4.641, 'eval_steps_per_second': 0.147, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m33%|      | 160/480 [1:31:37<2:54:36, 32.74s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 161/480 [1:32:11<3:17:58, 37.24s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 162/480 [1:32:45<3:10:49, 36.00s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 163/480 [1:33:18<3:05:57, 35.20s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 164/480 [1:33:51<3:01:55, 34.54s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 165/480 [1:34:24<2:59:46, 34.24s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 166/480 [1:34:59<2:59:12, 34.24s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 167/480 [1:35:32<2:56:39, 33.86s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 168/480 [1:36:05<2:55:27, 33.74s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 169/480 [1:36:39<2:54:29, 33.66s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 170/480 [1:37:11<2:52:40, 33.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0143, 'learning_rate': 6e-06, 'epoch': 10.62}\u001b[0m\n",
      "\u001b[34m35%|      | 170/480 [1:37:11<2:52:40, 33.42s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 171/480 [1:37:45<2:52:32, 33.50s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 172/480 [1:38:18<2:50:46, 33.27s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 173/480 [1:38:52<2:50:59, 33.42s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 174/480 [1:39:26<2:51:22, 33.60s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 175/480 [1:39:59<2:50:09, 33.47s/it]\u001b[0m\n",
      "\u001b[34m37%|      | 176/480 [1:40:32<2:48:54, 33.34s/it]\u001b[0m\n",
      "\u001b[34m37%|      | 177/480 [1:41:06<2:49:13, 33.51s/it]\u001b[0m\n",
      "\u001b[34m37%|      | 178/480 [1:41:38<2:47:17, 33.24s/it]\u001b[0m\n",
      "\u001b[34m37%|      | 179/480 [1:42:12<2:47:45, 33.44s/it]\u001b[0m\n",
      "\u001b[34m38%|      | 180/480 [1:42:45<2:46:38, 33.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0129, 'learning_rate': 6e-06, 'epoch': 11.25}\u001b[0m\n",
      "\u001b[34m38%|      | 180/480 [1:42:45<2:46:38, 33.33s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:26:49,145 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:26:49,145 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:26:49,148 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:26:49,148 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:26:49,148 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:26:49,148 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.68s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.91796875, 'eval_runtime': 14.8797, 'eval_samples_per_second': 4.234, 'eval_steps_per_second': 0.134, 'epoch': 11.25}\u001b[0m\n",
      "\u001b[34m38%|      | 180/480 [1:43:00<2:46:38, 33.33s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.68s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 181/480 [1:43:33<3:07:28, 37.62s/it]\u001b[0m\n",
      "\u001b[34m38%|      | 182/480 [1:44:07<3:01:48, 36.60s/it]\u001b[0m\n",
      "\u001b[34m38%|      | 183/480 [1:44:40<2:55:13, 35.40s/it]\u001b[0m\n",
      "\u001b[34m38%|      | 184/480 [1:45:14<2:53:01, 35.07s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 185/480 [1:45:47<2:49:17, 34.43s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 186/480 [1:46:20<2:46:38, 34.01s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 187/480 [1:46:54<2:45:56, 33.98s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 188/480 [1:47:28<2:44:52, 33.88s/it]\u001b[0m\n",
      "\u001b[34m39%|      | 189/480 [1:48:02<2:44:52, 34.00s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 190/480 [1:48:35<2:43:43, 33.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0103, 'learning_rate': 6e-06, 'epoch': 11.88}\u001b[0m\n",
      "\u001b[34m40%|      | 190/480 [1:48:35<2:43:43, 33.87s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 191/480 [1:49:10<2:43:26, 33.93s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 192/480 [1:49:42<2:40:33, 33.45s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 193/480 [1:50:16<2:40:53, 33.64s/it]\u001b[0m\n",
      "\u001b[34m40%|      | 194/480 [1:50:50<2:40:42, 33.71s/it]\u001b[0m\n",
      "\u001b[34m41%|      | 195/480 [1:51:23<2:39:17, 33.53s/it]\u001b[0m\n",
      "\u001b[34m41%|      | 196/480 [1:51:57<2:39:17, 33.65s/it]\u001b[0m\n",
      "\u001b[34m41%|      | 197/480 [1:52:30<2:38:25, 33.59s/it]\u001b[0m\n",
      "\u001b[34m41%|     | 198/480 [1:53:03<2:37:11, 33.45s/it]\u001b[0m\n",
      "\u001b[34m41%|     | 199/480 [1:53:37<2:37:28, 33.63s/it]\u001b[0m\n",
      "\u001b[34m42%|     | 200/480 [1:54:12<2:38:03, 33.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0084, 'learning_rate': 6e-06, 'epoch': 12.5}\u001b[0m\n",
      "\u001b[34m42%|     | 200/480 [1:54:12<2:38:03, 33.87s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:38:15,722 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:38:15,722 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:38:15,723 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:38:15,723 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:38:15,724 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:38:15,724 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.67s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.0546875, 'eval_runtime': 14.8633, 'eval_samples_per_second': 4.239, 'eval_steps_per_second': 0.135, 'epoch': 12.5}\u001b[0m\n",
      "\u001b[34m42%|     | 200/480 [1:54:27<2:38:03, 33.87s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.67s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 201/480 [1:55:01<2:58:23, 38.36s/it]\u001b[0m\n",
      "\u001b[34m42%|     | 202/480 [1:55:35<2:51:36, 37.04s/it]\u001b[0m\n",
      "\u001b[34m42%|     | 203/480 [1:56:09<2:46:34, 36.08s/it]\u001b[0m\n",
      "\u001b[34m42%|     | 204/480 [1:56:42<2:41:45, 35.17s/it]\u001b[0m\n",
      "\u001b[34m43%|     | 205/480 [1:57:14<2:37:22, 34.34s/it]\u001b[0m\n",
      "\u001b[34m43%|     | 206/480 [1:57:48<2:36:11, 34.20s/it]\u001b[0m\n",
      "\u001b[34m43%|     | 207/480 [1:58:22<2:36:00, 34.29s/it]\u001b[0m\n",
      "\u001b[34m43%|     | 208/480 [1:58:55<2:33:51, 33.94s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 209/480 [1:59:29<2:33:03, 33.89s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 210/480 [2:00:03<2:32:56, 33.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0075, 'learning_rate': 6e-06, 'epoch': 13.12}\u001b[0m\n",
      "\u001b[34m44%|     | 210/480 [2:00:03<2:32:56, 33.99s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 211/480 [2:00:37<2:32:03, 33.91s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 212/480 [2:01:10<2:30:16, 33.64s/it]\u001b[0m\n",
      "\u001b[34m44%|     | 213/480 [2:01:43<2:28:44, 33.43s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 214/480 [2:02:18<2:29:54, 33.81s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 215/480 [2:02:52<2:30:00, 33.96s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 216/480 [2:03:27<2:30:08, 34.12s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 217/480 [2:04:00<2:28:24, 33.86s/it]\u001b[0m\n",
      "\u001b[34m45%|     | 218/480 [2:04:33<2:27:08, 33.69s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 219/480 [2:05:07<2:27:00, 33.79s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 220/480 [2:05:40<2:25:33, 33.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0071, 'learning_rate': 6e-06, 'epoch': 13.75}\u001b[0m\n",
      "\u001b[34m46%|     | 220/480 [2:05:40<2:25:33, 33.59s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:49:44,193 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 04:49:44,193 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:49:44,193 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:49:44,193 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 04:49:44,193 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 04:49:44,193 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.07421875, 'eval_runtime': 14.6584, 'eval_samples_per_second': 4.298, 'eval_steps_per_second': 0.136, 'epoch': 13.75}\u001b[0m\n",
      "\u001b[34m46%|     | 220/480 [2:05:55<2:25:33, 33.59s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 221/480 [2:06:28<2:42:53, 37.74s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 222/480 [2:07:02<2:37:27, 36.62s/it]\u001b[0m\n",
      "\u001b[34m46%|     | 223/480 [2:07:35<2:33:02, 35.73s/it]\u001b[0m\n",
      "\u001b[34m47%|     | 224/480 [2:08:07<2:26:52, 34.42s/it]\u001b[0m\n",
      "\u001b[34m47%|     | 225/480 [2:08:41<2:25:25, 34.22s/it]\u001b[0m\n",
      "\u001b[34m47%|     | 226/480 [2:09:14<2:23:34, 33.92s/it]\u001b[0m\n",
      "\u001b[34m47%|     | 227/480 [2:09:47<2:22:33, 33.81s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 228/480 [2:10:20<2:20:33, 33.46s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 229/480 [2:10:54<2:20:59, 33.70s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 230/480 [2:11:28<2:20:11, 33.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0062, 'learning_rate': 6e-06, 'epoch': 14.38}\u001b[0m\n",
      "\u001b[34m48%|     | 230/480 [2:11:28<2:20:11, 33.65s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 231/480 [2:12:01<2:18:53, 33.47s/it]\u001b[0m\n",
      "\u001b[34m48%|     | 232/480 [2:12:34<2:18:26, 33.49s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 233/480 [2:13:08<2:18:09, 33.56s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 234/480 [2:13:42<2:18:11, 33.71s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 235/480 [2:14:15<2:16:43, 33.48s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 236/480 [2:14:49<2:17:03, 33.70s/it]\u001b[0m\n",
      "\u001b[34m49%|     | 237/480 [2:15:23<2:16:30, 33.71s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 238/480 [2:15:56<2:15:00, 33.47s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 239/480 [2:16:29<2:14:17, 33.43s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 240/480 [2:17:01<2:12:17, 33.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.006, 'learning_rate': 6e-06, 'epoch': 15.0}\u001b[0m\n",
      "\u001b[34m50%|     | 240/480 [2:17:02<2:12:17, 33.07s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:01:05,343 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:01:05,343 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:01:05,346 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:01:05,346 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:01:05,346 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:01:05,346 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.12890625, 'eval_runtime': 13.185, 'eval_samples_per_second': 4.778, 'eval_steps_per_second': 0.152, 'epoch': 15.0}\u001b[0m\n",
      "\u001b[34m50%|     | 240/480 [2:17:15<2:12:17, 33.07s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:06<00:00,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 241/480 [2:17:47<2:26:45, 36.84s/it]\u001b[0m\n",
      "\u001b[34m50%|     | 242/480 [2:18:20<2:21:19, 35.63s/it]\u001b[0m\n",
      "\u001b[34m51%|     | 243/480 [2:18:53<2:18:04, 34.96s/it]\u001b[0m\n",
      "\u001b[34m51%|     | 244/480 [2:19:27<2:15:28, 34.44s/it]\u001b[0m\n",
      "\u001b[34m51%|     | 245/480 [2:19:59<2:12:50, 33.92s/it]\u001b[0m\n",
      "\u001b[34m51%|    | 246/480 [2:20:32<2:11:13, 33.65s/it]\u001b[0m\n",
      "\u001b[34m51%|    | 247/480 [2:21:05<2:09:28, 33.34s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 248/480 [2:21:38<2:08:26, 33.22s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 249/480 [2:22:11<2:07:58, 33.24s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 250/480 [2:22:44<2:07:18, 33.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0049, 'learning_rate': 6e-06, 'epoch': 15.62}\u001b[0m\n",
      "\u001b[34m52%|    | 250/480 [2:22:44<2:07:18, 33.21s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 251/480 [2:23:19<2:07:56, 33.52s/it]\u001b[0m\n",
      "\u001b[34m52%|    | 252/480 [2:23:52<2:07:38, 33.59s/it]\u001b[0m\n",
      "\u001b[34m53%|    | 253/480 [2:24:26<2:06:52, 33.54s/it]\u001b[0m\n",
      "\u001b[34m53%|    | 254/480 [2:24:59<2:05:59, 33.45s/it]\u001b[0m\n",
      "\u001b[34m53%|    | 255/480 [2:25:32<2:04:51, 33.29s/it]\u001b[0m\n",
      "\u001b[34m53%|    | 256/480 [2:26:04<2:02:55, 32.93s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 257/480 [2:26:38<2:03:37, 33.26s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 258/480 [2:27:12<2:03:24, 33.35s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 259/480 [2:27:45<2:02:38, 33.30s/it]\u001b[0m\n",
      "\u001b[34m54%|    | 260/480 [2:28:17<2:01:04, 33.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0052, 'learning_rate': 6e-06, 'epoch': 16.25}\u001b[0m\n",
      "\u001b[34m54%|    | 260/480 [2:28:17<2:01:04, 33.02s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:12:20,924 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:12:20,924 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:12:20,927 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:12:20,927 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:12:20,928 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:12:20,928 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.171875, 'eval_runtime': 15.0456, 'eval_samples_per_second': 4.187, 'eval_steps_per_second': 0.133, 'epoch': 16.25}\u001b[0m\n",
      "\u001b[34m54%|    | 260/480 [2:28:32<2:01:04, 33.02s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 261/480 [2:29:06<2:17:38, 37.71s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 262/480 [2:29:39<2:12:27, 36.46s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 263/480 [2:30:13<2:08:26, 35.51s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 264/480 [2:30:47<2:06:11, 35.05s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 265/480 [2:31:19<2:03:13, 34.39s/it]\u001b[0m\n",
      "\u001b[34m55%|    | 266/480 [2:31:53<2:02:07, 34.24s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 267/480 [2:32:26<1:59:44, 33.73s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 268/480 [2:32:59<1:58:56, 33.66s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 269/480 [2:33:33<1:58:37, 33.73s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 270/480 [2:34:06<1:57:14, 33.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0047, 'learning_rate': 6e-06, 'epoch': 16.88}\u001b[0m\n",
      "\u001b[34m56%|    | 270/480 [2:34:06<1:57:14, 33.50s/it]\u001b[0m\n",
      "\u001b[34m56%|    | 271/480 [2:34:40<1:56:39, 33.49s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 272/480 [2:35:12<1:54:36, 33.06s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 273/480 [2:35:46<1:55:03, 33.35s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 274/480 [2:36:19<1:54:45, 33.42s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 275/480 [2:36:52<1:53:41, 33.28s/it]\u001b[0m\n",
      "\u001b[34m57%|    | 276/480 [2:37:25<1:53:05, 33.26s/it]\u001b[0m\n",
      "\u001b[34m58%|    | 277/480 [2:37:57<1:51:09, 32.85s/it]\u001b[0m\n",
      "\u001b[34m58%|    | 278/480 [2:38:31<1:51:01, 32.98s/it]\u001b[0m\n",
      "\u001b[34m58%|    | 279/480 [2:39:03<1:49:51, 32.80s/it]\u001b[0m\n",
      "\u001b[34m58%|    | 280/480 [2:39:37<1:50:52, 33.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0046, 'learning_rate': 6e-06, 'epoch': 17.5}\u001b[0m\n",
      "\u001b[34m58%|    | 280/480 [2:39:37<1:50:52, 33.26s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:23:41,200 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:23:41,200 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:23:41,200 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:23:41,200 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:23:41,200 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:23:41,200 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.68s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.20703125, 'eval_runtime': 14.6863, 'eval_samples_per_second': 4.29, 'eval_steps_per_second': 0.136, 'epoch': 17.5}\u001b[0m\n",
      "\u001b[34m58%|    | 280/480 [2:39:52<1:50:52, 33.26s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.68s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 281/480 [2:40:26<2:05:34, 37.86s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 282/480 [2:40:59<2:00:03, 36.38s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 283/480 [2:41:33<1:57:15, 35.71s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 284/480 [2:42:07<1:55:06, 35.24s/it]\u001b[0m\n",
      "\u001b[34m59%|    | 285/480 [2:42:40<1:52:37, 34.66s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 286/480 [2:43:14<1:50:42, 34.24s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 287/480 [2:43:47<1:48:45, 33.81s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 288/480 [2:44:20<1:47:30, 33.60s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 289/480 [2:44:53<1:46:27, 33.44s/it]\u001b[0m\n",
      "\u001b[34m60%|    | 290/480 [2:45:26<1:45:41, 33.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0044, 'learning_rate': 6e-06, 'epoch': 18.12}\u001b[0m\n",
      "\u001b[34m60%|    | 290/480 [2:45:26<1:45:41, 33.38s/it]\u001b[0m\n",
      "\u001b[34m61%|    | 291/480 [2:46:00<1:45:19, 33.43s/it]\u001b[0m\n",
      "\u001b[34m61%|    | 292/480 [2:46:34<1:45:38, 33.72s/it]\u001b[0m\n",
      "\u001b[34m61%|    | 293/480 [2:47:08<1:45:09, 33.74s/it]\u001b[0m\n",
      "\u001b[34m61%|   | 294/480 [2:47:41<1:44:11, 33.61s/it]\u001b[0m\n",
      "\u001b[34m61%|   | 295/480 [2:48:15<1:44:03, 33.75s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 296/480 [2:48:48<1:43:00, 33.59s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 297/480 [2:49:21<1:41:44, 33.36s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 298/480 [2:49:55<1:42:03, 33.65s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 299/480 [2:50:29<1:40:59, 33.48s/it]\u001b[0m\n",
      "\u001b[34m62%|   | 300/480 [2:51:02<1:40:33, 33.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0041, 'learning_rate': 6e-06, 'epoch': 18.75}\u001b[0m\n",
      "\u001b[34m62%|   | 300/480 [2:51:02<1:40:33, 33.52s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:35:05,993 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:35:05,993 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:35:05,993 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:35:05,993 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:35:05,993 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:35:05,993 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.20703125, 'eval_runtime': 14.9959, 'eval_samples_per_second': 4.201, 'eval_steps_per_second': 0.133, 'epoch': 18.75}\u001b[0m\n",
      "\u001b[34m62%|   | 300/480 [2:51:17<1:40:33, 33.52s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 301/480 [2:51:51<1:54:06, 38.25s/it]\u001b[0m\n",
      "\u001b[34m63%|   | 302/480 [2:52:25<1:49:12, 36.81s/it]\u001b[0m\n",
      "\u001b[34m63%|   | 303/480 [2:52:58<1:45:20, 35.71s/it]\u001b[0m\n",
      "\u001b[34m63%|   | 304/480 [2:53:30<1:41:44, 34.68s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 305/480 [2:54:04<1:40:08, 34.33s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 306/480 [2:54:37<1:38:47, 34.07s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 307/480 [2:55:12<1:38:24, 34.13s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 308/480 [2:55:45<1:37:24, 33.98s/it]\u001b[0m\n",
      "\u001b[34m64%|   | 309/480 [2:56:18<1:36:08, 33.73s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 310/480 [2:56:51<1:35:00, 33.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0041, 'learning_rate': 6e-06, 'epoch': 19.38}\u001b[0m\n",
      "\u001b[34m65%|   | 310/480 [2:56:51<1:35:00, 33.53s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 311/480 [2:57:24<1:34:05, 33.40s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 312/480 [2:57:58<1:33:32, 33.41s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 313/480 [2:58:31<1:33:04, 33.44s/it]\u001b[0m\n",
      "\u001b[34m65%|   | 314/480 [2:59:04<1:31:59, 33.25s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 315/480 [2:59:38<1:31:56, 33.43s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 316/480 [3:00:10<1:30:22, 33.07s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 317/480 [3:00:44<1:30:05, 33.16s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 318/480 [3:01:17<1:29:44, 33.24s/it]\u001b[0m\n",
      "\u001b[34m66%|   | 319/480 [3:01:50<1:28:39, 33.04s/it]\u001b[0m\n",
      "\u001b[34m67%|   | 320/480 [3:02:22<1:27:27, 32.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.004, 'learning_rate': 6e-06, 'epoch': 20.0}\u001b[0m\n",
      "\u001b[34m67%|   | 320/480 [3:02:22<1:27:27, 32.79s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:46:25,746 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:46:25,746 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:46:25,746 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:46:25,746 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:46:25,746 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:46:25,746 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.22265625, 'eval_runtime': 14.376, 'eval_samples_per_second': 4.382, 'eval_steps_per_second': 0.139, 'epoch': 20.0}\u001b[0m\n",
      "\u001b[34m67%|   | 320/480 [3:02:36<1:27:27, 32.79s/it]\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 321/480 [3:03:10<1:39:01, 37.37s/it]\u001b[0m\n",
      "\u001b[34m67%|   | 322/480 [3:03:43<1:35:06, 36.11s/it]\u001b[0m\n",
      "\u001b[34m67%|   | 323/480 [3:04:17<1:33:06, 35.58s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 324/480 [3:04:51<1:31:02, 35.01s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 325/480 [3:05:24<1:28:50, 34.39s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 326/480 [3:05:58<1:28:14, 34.38s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 327/480 [3:06:32<1:27:12, 34.20s/it]\u001b[0m\n",
      "\u001b[34m68%|   | 328/480 [3:07:06<1:25:58, 33.94s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 329/480 [3:07:39<1:25:08, 33.83s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 330/480 [3:08:12<1:23:37, 33.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0036, 'learning_rate': 6e-06, 'epoch': 20.62}\u001b[0m\n",
      "\u001b[34m69%|   | 330/480 [3:08:12<1:23:37, 33.45s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 331/480 [3:08:45<1:23:06, 33.47s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 332/480 [3:09:19<1:22:44, 33.55s/it]\u001b[0m\n",
      "\u001b[34m69%|   | 333/480 [3:09:52<1:21:53, 33.42s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 334/480 [3:10:26<1:21:56, 33.68s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 335/480 [3:10:59<1:20:57, 33.50s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 336/480 [3:11:33<1:20:21, 33.48s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 337/480 [3:12:07<1:20:17, 33.69s/it]\u001b[0m\n",
      "\u001b[34m70%|   | 338/480 [3:12:41<1:19:57, 33.79s/it]\u001b[0m\n",
      "\u001b[34m71%|   | 339/480 [3:13:13<1:18:20, 33.34s/it]\u001b[0m\n",
      "\u001b[34m71%|   | 340/480 [3:13:47<1:17:44, 33.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0038, 'learning_rate': 6e-06, 'epoch': 21.25}\u001b[0m\n",
      "\u001b[34m71%|   | 340/480 [3:13:47<1:17:44, 33.32s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:57:50,461 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 05:57:50,461 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:57:50,462 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 05:57:50,462 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:57:50,463 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 05:57:50,463 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.234375, 'eval_runtime': 14.8762, 'eval_samples_per_second': 4.235, 'eval_steps_per_second': 0.134, 'epoch': 21.25}\u001b[0m\n",
      "\u001b[34m71%|   | 340/480 [3:14:02<1:17:44, 33.32s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 341/480 [3:14:35<1:27:50, 37.91s/it]\u001b[0m\n",
      "\u001b[34m71%|  | 342/480 [3:15:09<1:24:06, 36.57s/it]\u001b[0m\n",
      "\u001b[34m71%|  | 343/480 [3:15:42<1:21:02, 35.49s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 344/480 [3:16:15<1:19:00, 34.86s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 345/480 [3:16:48<1:16:59, 34.22s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 346/480 [3:17:22<1:16:16, 34.15s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 347/480 [3:17:55<1:14:59, 33.83s/it]\u001b[0m\n",
      "\u001b[34m72%|  | 348/480 [3:18:28<1:14:15, 33.75s/it]\u001b[0m\n",
      "\u001b[34m73%|  | 349/480 [3:19:02<1:13:23, 33.61s/it]\u001b[0m\n",
      "\u001b[34m73%|  | 350/480 [3:19:35<1:12:34, 33.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0038, 'learning_rate': 6e-06, 'epoch': 21.88}\u001b[0m\n",
      "\u001b[34m73%|  | 350/480 [3:19:35<1:12:34, 33.50s/it]\u001b[0m\n",
      "\u001b[34m73%|  | 351/480 [3:20:10<1:12:42, 33.82s/it]\u001b[0m\n",
      "\u001b[34m73%|  | 352/480 [3:20:43<1:11:52, 33.69s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 353/480 [3:21:16<1:11:03, 33.57s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 354/480 [3:21:49<1:09:43, 33.20s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 355/480 [3:22:22<1:09:34, 33.40s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 356/480 [3:22:56<1:09:26, 33.60s/it]\u001b[0m\n",
      "\u001b[34m74%|  | 357/480 [3:23:29<1:08:30, 33.42s/it]\u001b[0m\n",
      "\u001b[34m75%|  | 358/480 [3:24:03<1:07:54, 33.40s/it]\u001b[0m\n",
      "\u001b[34m75%|  | 359/480 [3:24:36<1:07:12, 33.33s/it]\u001b[0m\n",
      "\u001b[34m75%|  | 360/480 [3:25:09<1:06:35, 33.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0037, 'learning_rate': 6e-06, 'epoch': 22.5}\u001b[0m\n",
      "\u001b[34m75%|  | 360/480 [3:25:09<1:06:35, 33.30s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:09:13,051 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:09:13,051 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:09:13,054 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:09:13,054 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:09:13,055 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:09:13,055 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.203125, 'eval_runtime': 15.1262, 'eval_samples_per_second': 4.165, 'eval_steps_per_second': 0.132, 'epoch': 22.5}\u001b[0m\n",
      "\u001b[34m75%|  | 360/480 [3:25:24<1:06:35, 33.30s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 361/480 [3:25:59<1:15:38, 38.14s/it]\u001b[0m\n",
      "\u001b[34m75%|  | 362/480 [3:26:32<1:12:03, 36.64s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 363/480 [3:27:04<1:09:05, 35.43s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 364/480 [3:27:37<1:07:09, 34.74s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 365/480 [3:28:10<1:05:34, 34.22s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 366/480 [3:28:44<1:04:23, 33.89s/it]\u001b[0m\n",
      "\u001b[34m76%|  | 367/480 [3:29:18<1:03:49, 33.89s/it]\u001b[0m\n",
      "\u001b[34m77%|  | 368/480 [3:29:49<1:01:53, 33.15s/it]\u001b[0m\n",
      "\u001b[34m77%|  | 369/480 [3:30:22<1:01:09, 33.06s/it]\u001b[0m\n",
      "\u001b[34m77%|  | 370/480 [3:30:55<1:00:45, 33.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0037, 'learning_rate': 6e-06, 'epoch': 23.12}\u001b[0m\n",
      "\u001b[34m77%|  | 370/480 [3:30:55<1:00:45, 33.14s/it]\u001b[0m\n",
      "\u001b[34m77%|  | 371/480 [3:31:29<1:00:34, 33.35s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 372/480 [3:32:02<59:52, 33.26s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 373/480 [3:32:36<59:45, 33.51s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 374/480 [3:33:10<59:23, 33.61s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 375/480 [3:33:44<58:56, 33.68s/it]\u001b[0m\n",
      "\u001b[34m78%|  | 376/480 [3:34:17<58:16, 33.62s/it]\u001b[0m\n",
      "\u001b[34m79%|  | 377/480 [3:34:51<57:44, 33.64s/it]\u001b[0m\n",
      "\u001b[34m79%|  | 378/480 [3:35:24<56:50, 33.43s/it]\u001b[0m\n",
      "\u001b[34m79%|  | 379/480 [3:35:57<55:52, 33.20s/it]\u001b[0m\n",
      "\u001b[34m79%|  | 380/480 [3:36:30<55:17, 33.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0035, 'learning_rate': 6e-06, 'epoch': 23.75}\u001b[0m\n",
      "\u001b[34m79%|  | 380/480 [3:36:30<55:17, 33.18s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:20:33,523 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:20:33,523 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:20:33,523 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:20:33,523 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:20:33,524 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:20:33,524 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.2421875, 'eval_runtime': 13.078, 'eval_samples_per_second': 4.817, 'eval_steps_per_second': 0.153, 'epoch': 23.75}\u001b[0m\n",
      "\u001b[34m79%|  | 380/480 [3:36:43<55:17, 33.18s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:06<00:00,  3.26s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 381/480 [3:37:16<1:01:05, 37.03s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 382/480 [3:37:49<58:25, 35.77s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 383/480 [3:38:22<56:33, 34.98s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 384/480 [3:38:53<54:17, 33.93s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 385/480 [3:39:27<53:28, 33.78s/it]\u001b[0m\n",
      "\u001b[34m80%|  | 386/480 [3:40:00<52:42, 33.64s/it]\u001b[0m\n",
      "\u001b[34m81%|  | 387/480 [3:40:33<51:53, 33.48s/it]\u001b[0m\n",
      "\u001b[34m81%|  | 388/480 [3:41:07<51:46, 33.77s/it]\u001b[0m\n",
      "\u001b[34m81%|  | 389/480 [3:41:41<51:05, 33.68s/it]\u001b[0m\n",
      "\u001b[34m81%| | 390/480 [3:42:14<50:20, 33.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0035, 'learning_rate': 6e-06, 'epoch': 24.38}\u001b[0m\n",
      "\u001b[34m81%| | 390/480 [3:42:14<50:20, 33.56s/it]\u001b[0m\n",
      "\u001b[34m81%| | 391/480 [3:42:48<49:56, 33.67s/it]\u001b[0m\n",
      "\u001b[34m82%| | 392/480 [3:43:21<49:14, 33.57s/it]\u001b[0m\n",
      "\u001b[34m82%| | 393/480 [3:43:54<48:18, 33.32s/it]\u001b[0m\n",
      "\u001b[34m82%| | 394/480 [3:44:28<47:50, 33.38s/it]\u001b[0m\n",
      "\u001b[34m82%| | 395/480 [3:45:02<47:31, 33.54s/it]\u001b[0m\n",
      "\u001b[34m82%| | 396/480 [3:45:35<46:55, 33.52s/it]\u001b[0m\n",
      "\u001b[34m83%| | 397/480 [3:46:09<46:37, 33.70s/it]\u001b[0m\n",
      "\u001b[34m83%| | 398/480 [3:46:43<46:05, 33.72s/it]\u001b[0m\n",
      "\u001b[34m83%| | 399/480 [3:47:16<45:24, 33.64s/it]\u001b[0m\n",
      "\u001b[34m83%| | 400/480 [3:47:49<44:34, 33.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0036, 'learning_rate': 6e-06, 'epoch': 25.0}\u001b[0m\n",
      "\u001b[34m83%| | 400/480 [3:47:49<44:34, 33.43s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:31:53,221 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:31:53,221 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:31:53,224 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:31:53,224 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:31:53,225 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:31:53,225 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.23046875, 'eval_runtime': 15.0059, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 0.133, 'epoch': 25.0}\u001b[0m\n",
      "\u001b[34m83%| | 400/480 [3:48:04<44:34, 33.43s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.70s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%| | 401/480 [3:48:38<50:00, 37.97s/it]\u001b[0m\n",
      "\u001b[34m84%| | 402/480 [3:49:11<47:33, 36.59s/it]\u001b[0m\n",
      "\u001b[34m84%| | 403/480 [3:49:45<45:52, 35.75s/it]\u001b[0m\n",
      "\u001b[34m84%| | 404/480 [3:50:18<44:15, 34.94s/it]\u001b[0m\n",
      "\u001b[34m84%| | 405/480 [3:50:52<43:08, 34.51s/it]\u001b[0m\n",
      "\u001b[34m85%| | 406/480 [3:51:25<42:09, 34.19s/it]\u001b[0m\n",
      "\u001b[34m85%| | 407/480 [3:51:58<41:16, 33.92s/it]\u001b[0m\n",
      "\u001b[34m85%| | 408/480 [3:52:31<40:21, 33.63s/it]\u001b[0m\n",
      "\u001b[34m85%| | 409/480 [3:53:05<39:56, 33.75s/it]\u001b[0m\n",
      "\u001b[34m85%| | 410/480 [3:53:38<39:08, 33.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0031, 'learning_rate': 6e-06, 'epoch': 25.62}\u001b[0m\n",
      "\u001b[34m85%| | 410/480 [3:53:38<39:08, 33.55s/it]\u001b[0m\n",
      "\u001b[34m86%| | 411/480 [3:54:11<38:23, 33.39s/it]\u001b[0m\n",
      "\u001b[34m86%| | 412/480 [3:54:45<37:48, 33.37s/it]\u001b[0m\n",
      "\u001b[34m86%| | 413/480 [3:55:18<37:15, 33.37s/it]\u001b[0m\n",
      "\u001b[34m86%| | 414/480 [3:55:52<36:44, 33.40s/it]\u001b[0m\n",
      "\u001b[34m86%| | 415/480 [3:56:25<36:07, 33.35s/it]\u001b[0m\n",
      "\u001b[34m87%| | 416/480 [3:56:55<34:39, 32.49s/it]\u001b[0m\n",
      "\u001b[34m87%| | 417/480 [3:57:28<34:17, 32.66s/it]\u001b[0m\n",
      "\u001b[34m87%| | 418/480 [3:58:02<33:58, 32.88s/it]\u001b[0m\n",
      "\u001b[34m87%| | 419/480 [3:58:35<33:29, 32.95s/it]\u001b[0m\n",
      "\u001b[34m88%| | 420/480 [3:59:09<33:17, 33.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0045, 'learning_rate': 6e-06, 'epoch': 26.25}\u001b[0m\n",
      "\u001b[34m88%| | 420/480 [3:59:09<33:17, 33.30s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:43:12,866 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:43:12,866 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:43:12,869 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:43:12,869 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:43:12,870 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:43:12,870 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.234375, 'eval_runtime': 15.0819, 'eval_samples_per_second': 4.177, 'eval_steps_per_second': 0.133, 'epoch': 26.25}\u001b[0m\n",
      "\u001b[34m88%| | 420/480 [3:59:24<33:17, 33.30s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:08<00:00,  3.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m88%| | 421/480 [3:59:58<37:18, 37.94s/it]\u001b[0m\n",
      "\u001b[34m88%| | 422/480 [4:00:31<35:25, 36.65s/it]\u001b[0m\n",
      "\u001b[34m88%| | 423/480 [4:01:05<34:04, 35.87s/it]\u001b[0m\n",
      "\u001b[34m88%| | 424/480 [4:01:38<32:39, 34.99s/it]\u001b[0m\n",
      "\u001b[34m89%| | 425/480 [4:02:12<31:35, 34.47s/it]\u001b[0m\n",
      "\u001b[34m89%| | 426/480 [4:02:45<30:36, 34.01s/it]\u001b[0m\n",
      "\u001b[34m89%| | 427/480 [4:03:17<29:41, 33.61s/it]\u001b[0m\n",
      "\u001b[34m89%| | 428/480 [4:03:50<28:57, 33.41s/it]\u001b[0m\n",
      "\u001b[34m89%| | 429/480 [4:04:24<28:24, 33.42s/it]\u001b[0m\n",
      "\u001b[34m90%| | 430/480 [4:04:58<28:01, 33.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0042, 'learning_rate': 6e-06, 'epoch': 26.88}\u001b[0m\n",
      "\u001b[34m90%| | 430/480 [4:04:58<28:01, 33.64s/it]\u001b[0m\n",
      "\u001b[34m90%| | 431/480 [4:05:31<27:23, 33.54s/it]\u001b[0m\n",
      "\u001b[34m90%| | 432/480 [4:06:04<26:43, 33.41s/it]\u001b[0m\n",
      "\u001b[34m90%| | 433/480 [4:06:38<26:11, 33.44s/it]\u001b[0m\n",
      "\u001b[34m90%| | 434/480 [4:07:10<25:27, 33.20s/it]\u001b[0m\n",
      "\u001b[34m91%| | 435/480 [4:07:44<24:58, 33.29s/it]\u001b[0m\n",
      "\u001b[34m91%| | 436/480 [4:08:17<24:21, 33.20s/it]\u001b[0m\n",
      "\u001b[34m91%| | 437/480 [4:08:50<23:52, 33.32s/it]\u001b[0m\n",
      "\u001b[34m91%|| 438/480 [4:09:24<23:20, 33.34s/it]\u001b[0m\n",
      "\u001b[34m91%|| 439/480 [4:09:58<22:55, 33.55s/it]\u001b[0m\n",
      "\u001b[34m92%|| 440/480 [4:10:30<22:09, 33.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0037, 'learning_rate': 6e-06, 'epoch': 27.5}\u001b[0m\n",
      "\u001b[34m92%|| 440/480 [4:10:30<22:09, 33.23s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:54:34,235 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 06:54:34,235 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:54:34,238 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 06:54:34,238 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:54:34,239 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 06:54:34,239 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.16796875, 'eval_runtime': 13.2152, 'eval_samples_per_second': 4.767, 'eval_steps_per_second': 0.151, 'epoch': 27.5}\u001b[0m\n",
      "\u001b[34m92%|| 440/480 [4:10:44<22:09, 33.23s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m92%|| 441/480 [4:11:17<24:09, 37.18s/it]\u001b[0m\n",
      "\u001b[34m92%|| 442/480 [4:11:49<22:39, 35.79s/it]\u001b[0m\n",
      "\u001b[34m92%|| 443/480 [4:12:22<21:30, 34.87s/it]\u001b[0m\n",
      "\u001b[34m92%|| 444/480 [4:12:54<20:27, 34.09s/it]\u001b[0m\n",
      "\u001b[34m93%|| 445/480 [4:13:28<19:47, 33.93s/it]\u001b[0m\n",
      "\u001b[34m93%|| 446/480 [4:14:00<18:59, 33.52s/it]\u001b[0m\n",
      "\u001b[34m93%|| 447/480 [4:14:34<18:28, 33.59s/it]\u001b[0m\n",
      "\u001b[34m93%|| 448/480 [4:15:06<17:39, 33.10s/it]\u001b[0m\n",
      "\u001b[34m94%|| 449/480 [4:15:39<17:08, 33.16s/it]\u001b[0m\n",
      "\u001b[34m94%|| 450/480 [4:16:13<16:40, 33.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0053, 'learning_rate': 6e-06, 'epoch': 28.12}\u001b[0m\n",
      "\u001b[34m94%|| 450/480 [4:16:13<16:40, 33.36s/it]\u001b[0m\n",
      "\u001b[34m94%|| 451/480 [4:16:47<16:07, 33.35s/it]\u001b[0m\n",
      "\u001b[34m94%|| 452/480 [4:17:20<15:31, 33.27s/it]\u001b[0m\n",
      "\u001b[34m94%|| 453/480 [4:17:53<14:55, 33.17s/it]\u001b[0m\n",
      "\u001b[34m95%|| 454/480 [4:18:27<14:27, 33.38s/it]\u001b[0m\n",
      "\u001b[34m95%|| 455/480 [4:18:59<13:51, 33.24s/it]\u001b[0m\n",
      "\u001b[34m95%|| 456/480 [4:19:33<13:20, 33.33s/it]\u001b[0m\n",
      "\u001b[34m95%|| 457/480 [4:20:06<12:47, 33.38s/it]\u001b[0m\n",
      "\u001b[34m95%|| 458/480 [4:20:39<12:11, 33.23s/it]\u001b[0m\n",
      "\u001b[34m96%|| 459/480 [4:21:13<11:40, 33.38s/it]\u001b[0m\n",
      "\u001b[34m96%|| 460/480 [4:21:45<10:59, 32.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0046, 'learning_rate': 6e-06, 'epoch': 28.75}\u001b[0m\n",
      "\u001b[34m96%|| 460/480 [4:21:45<10:59, 32.99s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:05:49,006 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:05:49,006 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:05:49,006 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:05:49,006 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:05:49,006 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:05:49,006 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:06<00:00,  3.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.1640625, 'eval_runtime': 12.8912, 'eval_samples_per_second': 4.887, 'eval_steps_per_second': 0.155, 'epoch': 28.75}\u001b[0m\n",
      "\u001b[34m96%|| 460/480 [4:21:58<10:59, 32.99s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:06<00:00,  3.21s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m96%|| 461/480 [4:22:30<11:33, 36.49s/it]\u001b[0m\n",
      "\u001b[34m96%|| 462/480 [4:23:03<10:39, 35.53s/it]\u001b[0m\n",
      "\u001b[34m96%|| 463/480 [4:23:36<09:51, 34.82s/it]\u001b[0m\n",
      "\u001b[34m97%|| 464/480 [4:24:09<09:05, 34.12s/it]\u001b[0m\n",
      "\u001b[34m97%|| 465/480 [4:24:42<08:26, 33.76s/it]\u001b[0m\n",
      "\u001b[34m97%|| 466/480 [4:25:16<07:53, 33.86s/it]\u001b[0m\n",
      "\u001b[34m97%|| 467/480 [4:25:48<07:15, 33.52s/it]\u001b[0m\n",
      "\u001b[34m98%|| 468/480 [4:26:21<06:38, 33.22s/it]\u001b[0m\n",
      "\u001b[34m98%|| 469/480 [4:26:53<06:02, 32.97s/it]\u001b[0m\n",
      "\u001b[34m98%|| 470/480 [4:27:26<05:29, 32.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0059, 'learning_rate': 6e-06, 'epoch': 29.38}\u001b[0m\n",
      "\u001b[34m98%|| 470/480 [4:27:26<05:29, 32.95s/it]\u001b[0m\n",
      "\u001b[34m98%|| 471/480 [4:28:00<04:58, 33.12s/it]\u001b[0m\n",
      "\u001b[34m98%|| 472/480 [4:28:33<04:24, 33.00s/it]\u001b[0m\n",
      "\u001b[34m99%|| 473/480 [4:29:05<03:50, 32.93s/it]\u001b[0m\n",
      "\u001b[34m99%|| 474/480 [4:29:38<03:16, 32.79s/it]\u001b[0m\n",
      "\u001b[34m99%|| 475/480 [4:30:11<02:44, 32.95s/it]\u001b[0m\n",
      "\u001b[34m99%|| 476/480 [4:30:45<02:13, 33.32s/it]\u001b[0m\n",
      "\u001b[34m99%|| 477/480 [4:31:17<01:38, 32.72s/it]\u001b[0m\n",
      "\u001b[34m100%|| 478/480 [4:31:51<01:06, 33.08s/it]\u001b[0m\n",
      "\u001b[34m100%|| 479/480 [4:32:24<00:33, 33.32s/it]\u001b[0m\n",
      "\u001b[34m100%|| 480/480 [4:32:56<00:00, 32.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0051, 'learning_rate': 6e-06, 'epoch': 30.0}\u001b[0m\n",
      "\u001b[34m100%|| 480/480 [4:32:56<00:00, 32.94s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:17:00,271 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:17:00,271 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:17:00,271 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:17:00,271 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:17:00,271 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:17:00,271 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.64s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.13671875, 'eval_runtime': 14.6667, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 0.136, 'epoch': 30.0}\u001b[0m\n",
      "\u001b[34m100%|| 480/480 [4:33:11<00:00, 32.94s/it]\u001b[0m\n",
      "\u001b[34m#015100%|| 2/2 [00:07<00:00,  3.64s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-09-05 07:17:14,938 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-09-05 07:17:14,938 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 16391.6223, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.029, 'train_loss': 0.28567816118399303, 'epoch': 30.0}\u001b[0m\n",
      "\u001b[34m100%|| 480/480 [4:33:11<00:00, 32.94s/it]\u001b[0m\n",
      "\u001b[34m100%|| 480/480 [4:33:11<00:00, 34.15s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2023-09-05 07:17:14,940 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2023-09-05 07:17:14,940 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-09-05 07:17:14,941 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-09-05 07:17:14,941 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-09-05 07:17:14,941 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-09-05 07:17:14,941 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-09-05 07:17:15,089 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-09-05 07:17:15,089 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-09-05 07:17:15,090 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-09-05 07:17:15,090 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-09-05 07:17:15,090 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-09-05 07:17:15,090 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:17:29,144] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step480 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:17:29,144] [INFO] [engine.py:3354:save_16bit_model] Saving model weights to /opt/ml/model/pytorch_model.bin, tag: global_step480\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:17:29,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:17:40,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:17:40,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step480 is ready now!\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       30.0\n",
      "  train_loss               =     0.2857\n",
      "  train_runtime            = 4:33:11.62\n",
      "  train_samples            =        252\n",
      "  train_samples_per_second =      0.461\n",
      "  train_steps_per_second   =      0.029\u001b[0m\n",
      "\u001b[34m09/05/2023 07:17:41 - INFO - __main__ -   Start Evaluation.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:17:41,201 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-09-05 07:17:41,201 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:17:41,201 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:17:41,201 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-09-05 07:17:41,201 >>   Num examples = 63\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-09-05 07:17:41,201 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  3.66s/it]\u001b[0m\n",
      "\u001b[34m100%|| 2/2 [00:07<00:00,  4.00s/it]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\n",
      "  epoch                   =       30.0\n",
      "  eval_loss               =     4.1367\n",
      "  eval_runtime            = 0:00:14.64\n",
      "  eval_samples            =         63\n",
      "  eval_samples_per_second =      4.303\n",
      "  eval_steps_per_second   =      0.137\n",
      "  perplexity              =    62.5971\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:18:01,126] [INFO] [launch.py:346:main] Process 165 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:18:01,126] [INFO] [launch.py:346:main] Process 164 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:18:01,126] [INFO] [launch.py:346:main] Process 163 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-09-05 07:18:01,126] [INFO] [launch.py:346:main] Process 166 exits successfully.\u001b[0m\n",
      "\u001b[34m2023-09-05 07:18:03,654 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-05 07:18:03,654 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-05 07:18:03,655 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-05 07:18:08 Uploading - Uploading generated training model\n",
      "2023-09-05 07:28:09 Completed - Training job completed\n",
      "Training seconds: 18093\n",
      "Billable seconds: 18093\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5396b2b4-ab76-4f8d-88ba-a0c17ee4186c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>2.5852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>2.3801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>2.1456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.7910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.4953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1740.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2100.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.8130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.5367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2760.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3120.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.1662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name   value\n",
       "0        0.0  train:loss  2.5852\n",
       "1      360.0  train:loss  2.3801\n",
       "2      720.0  train:loss  2.1456\n",
       "3     1020.0  train:loss  1.7910\n",
       "4     1380.0  train:loss  1.4953\n",
       "5     1740.0  train:loss  1.1247\n",
       "6     2100.0  train:loss  0.8130\n",
       "7     2400.0  train:loss  0.5367\n",
       "8     2760.0  train:loss  0.2545\n",
       "9     3120.0  train:loss  0.1662"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1498b6d-6c01-4573-80db-f49a712d5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. TODO\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994c7382-eb21-4602-8b02-16c8645ff1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-09-05-07-28-45-903\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-huggingface-textgener-2023-09-05-07-28-45-903\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-huggingface-textgener-2023-09-05-07-28-45-903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"jumpstart-example-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a71ae3-d17b-444d-89a3-e4ce0811109b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76bed497-f620-422b-89b9-8db657dc4d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_trained_model(question):\n",
    "    parameters = {\n",
    "        \"max_length\": 250,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 150,\n",
    "        \"top_p\": 0.8,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1,\n",
    "    }\n",
    "    \n",
    "    endpoint_name = \"pre-trained-model-endpoint\"\n",
    "\n",
    "    res_gpt_before_finetune = []\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{question}:\", **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    print(\"Pre-trained model response:\")\n",
    "    print(\"\\n\")\n",
    "    print(generated_texts)\n",
    "    print(\"-------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9826367-cc46-45bb-a9f3-2a306dfb2ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tuned_model(question):\n",
    "    parameters = {\n",
    "        \"max_length\": 250,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 150,\n",
    "        \"top_p\": 0.8,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1,\n",
    "    }\n",
    "\n",
    "    endpoint_name = \"fine-tuned-model-endpoint\"\n",
    "    res_gpt_before_finetune = []\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{question}:\", **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    print(\"Fine-tuned model response:\")\n",
    "    print(\"\\n\")\n",
    "    print(generated_texts)\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f091762-a564-4a11-b8dd-91238678cc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model response:\n",
      "\n",
      "\n",
      "What motivates Captain Ahab's relentless pursuit of Moby Dick throughout the novel?:\n",
      "\n",
      "\n",
      "Ahab has driven his family from their home. His family, and most of his crew have died at sea or died as prisoners of war. Ahab, alone in a universe without God, becomes the devil himself. He is the only one who sees himself as the only one in the universe.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ahab is a character that is a man of few friends, if any at all. Most of the time he is a solitary and angry person, except for the time he spends with the crew of the Pequod. It is this crew that is his best friend, and family.\n",
      "\n",
      "Ahab has been given the name of white whale and is a great enemy of white whale. He is seen as the only one to ever find white whale in his nature.\n",
      "Ahab and the white whale are one of the most important character in Moby Dick. The white whale is a metaphor for God and for white whale represents Ahab's hatred for God, as he says in the novel that the white whale killed him as the devil.\n",
      "\n",
      "Ahab's quest to kill the white whale was for revenge and\n",
      "-------------------------------\n",
      "Fine-tuned model response:\n",
      "\n",
      "\n",
      "What motivates Captain Ahab's relentless pursuit of Moby Dick throughout the novel?:In this chapter, Ahab's pursuit of Moby Dick is described. His primary motive seems to be vengeance for the White Whale's murderous rampage on the Indian ship, the Pequod, and its crew. But there is more to it than that. As the White Whale has seemingly marked him for death, Ahab seems to be in a perpetual state of near-mutual annihilative fixation with the monster. As Ahab once told Starbuck, he would rather forget he had ever known the world, than know that he had ever forgotten the White Whale. As Ahab is consumed with the monomania to capture or kill the monster, the monster is consumed with the monomania to evade and elude him. This mutual monomania constitutes their life-and-death struggle. It is a queer coexisting monomania that exists between these two enemies; and it is this coexisting monomania that makes them play such an implacable game of blindman's-bluff with each other, and with themselves. For the more Ahab chases and pursues the whale, the more the whale eludes him; and\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What motivates Captain Ahab's relentless pursuit of Moby Dick throughout the novel?\"\n",
    "\n",
    "pre_trained_model(question)\n",
    "fine_tuned_model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d7933bf-28b5-4683-939d-d10bbc9b8f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model response:\n",
      "\n",
      "\n",
      "Analyze the character of Queequeg and his role in the novel named Moby Dick. What does he represent?: what does he represent to his character and the novel? How does this reflect upon the concept of the American Dream?: what does this concept mean in the context of the book? Does the book or any of the characters seem to be like what we are all too familiar with? Discuss with other students and learn more about the concept of \"The American Dream\" and what the book is about. Themes of the book? Discuss with your class what you learned from the novel and how this relates to your personal life and how it may relate to you.\n",
      "-------------------------------\n",
      "Fine-tuned model response:\n",
      "\n",
      "\n",
      "Analyze the character of Queequeg and his role in the novel named Moby Dick. What does he represent?: Is he good or evil?\n",
      "\n",
      "I will now proceed to analyze the character of Queequeg, and his role in the book named Moby Dick. I will begin by saying that he represents an ancient and honorable nation, and I will call him Queequeg, the Fourth. He is a native of the central region of the continent, and is thus a denizen of the land of the Fourth. He has always been faithful to the Fourth, and he will be found faithful to it in this, his latest adventure. I will now proceed to give a more full analysis of Queequeg. First, he is an honorable man. Second, he is a high-minded man. Third, he is a man of faith. Fourth, he is a man of his word. Fifth, he is a man who always keeps his promises. Sixth, he is a generous man. Seventh, he is an uncompromising man. Eighth, he is a man who never changes his beliefs or his ideals. Ninth, he is a man who never fights but if it is perfectly just and righteous. Tenth, he\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"Analyze the character of Queequeg and his role in the novel named Moby Dick. What does he represent?\"\n",
    "\n",
    "pre_trained_model(question)\n",
    "fine_tuned_model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "752d03fd-61ec-456e-be04-6168f4c3004d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model response:\n",
      "\n",
      "\n",
      "Analyze the character named Ahab and his role from the novel named Moby Dick?: The Whale, Part 2\n",
      "\n",
      "How would you evaluate his role in the novel Moby Dick\n",
      "\n",
      "How would you interpret his role\n",
      "His role in the novel?\n",
      "Can you think about it as\n",
      "A character\n",
      "In his role and what role\n",
      "How would you analyze his role in the novel?\n",
      "Would you like to give me your opinion?\n",
      "Is it true that he is\n",
      "It's been discussed in the novel?\n",
      "Novel?\n",
      "Yes, I mean\n",
      "It's been discussed?\n",
      "You know, it's in the novel, and it's been\n",
      "What role that has been discussed?\n",
      "In the novel, you have to analyze?\n",
      "What are the points that are discussed\n",
      "in the novel?\n",
      "Yes, but you can say\n",
      "In the novel\n",
      "In the novel\n",
      "Yes, you can discuss it, but\n",
      "You can have a different opinion.\n",
      "\n",
      "A, but you can\n",
      "think about it, can you can discuss it, but you can have a different opinion about it, so\n",
      "\n",
      "But if I can have different opinion about it?\n",
      "Can you have a different\n",
      "\n",
      "\n",
      "What is\n",
      "-------------------------------\n",
      "Fine-tuned model response:\n",
      "\n",
      "\n",
      "Analyze the character named Ahab and his role from the novel named Moby Dick?: (Include reasons and examples for your response.)\n",
      "\n",
      "Ahab is a very powerful man. In the novel, he is the captain of a whaling ship called the Pequod. His character has many different aspects. He is a man who wants to kill Moby Dick, a white whale that he sees as his mortal enemy. Because of this, he has many different personality traits. One of the traits is that he is very controlling. He is the captain of the ship Pequod. Because of this, he is the one who makes all the decisions about what the ship will do. Also, he has a lot of authority over the crew. Because of all of this, he can be very demanding of his crew. He has a lot of authority over them, and he can be very demanding.\n",
      "\n",
      "Ahab is also very self-centered. He is very focused on himself and what he wants. This is why he is so controlling. He wants to control everything that is going on on the ship. He wants to control the crew, the ship, and the whale. He wants everything to go his way. He has\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"Analyze the character named Ahab and his role from the novel named Moby Dick?\"\n",
    "\n",
    "pre_trained_model(question)\n",
    "fine_tuned_model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70c646-bb77-4648-9985-bce5bfd2705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
